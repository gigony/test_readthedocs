
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Modules Overview &#8212; MONAI Deploy Application SDK 0.1.0 Documentation</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API Reference" href="api.html" />
    <link rel="prev" title="What’s new in 0.5" href="whatsnew_0_5.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="index.html">
  <img src="_static/MONAI-logo-color.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="whatsnew.html">
  What’s New
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  Modules Overview
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="api.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="installation.html">
  Installation Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="contrib.html">
  Development
 </a>
</li>

    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://github.com/Project-MONAI/tutorials">Tutorials<i class="fas fa-external-link-alt"></i></a>
    </li>
    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/project-monai/monai" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/projectmonai" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#monai-architecture">
   MONAI architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#medical-image-data-i-o-processing-and-augmentation">
   Medical image data I/O, processing and augmentation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transforms-support-both-dictionary-and-array-format-data">
     1. Transforms support both Dictionary and Array format data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#medical-specific-transforms">
     2. Medical specific transforms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fused-spatial-transforms-and-gpu-acceleration">
     3. Fused spatial transforms and GPU acceleration
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#randomly-crop-out-batch-images-based-on-positive-negative-ratio">
     4. Randomly crop out batch images based on positive/negative ratio
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deterministic-training-for-reproducibility">
     5. Deterministic training for reproducibility
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-transform-chains">
     6. Multiple transform chains
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#debug-transforms-with-datastats">
     7. Debug transforms with DataStats
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#post-processing-transforms-for-model-output">
     8. Post-processing transforms for model output
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#integrate-third-party-transforms">
     9. Integrate third-party transforms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#io-factory-for-medical-image-formats">
     10. IO factory for medical image formats
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-transform-data-into-nifti-or-png-files">
     11. Save transform data into NIfTI or PNG files
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#automatically-ensure-channel-first-data-shape">
     12. Automatically ensure
     <code class="docutils literal notranslate">
      <span class="pre">
       channel-first
      </span>
     </code>
     data shape
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#invert-spatial-transforms-and-test-time-augmentations">
     13. Invert spatial transforms and test-time augmentations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#datasets">
   Datasets
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cache-io-and-transforms-data-to-accelerate-training">
     1. Cache IO and transforms data to accelerate training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cache-intermediate-outcomes-into-persistent-storage">
     2. Cache intermediate outcomes into persistent storage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#smartcache-mechanism-for-big-datasets">
     3. SmartCache mechanism for big datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#zip-multiple-pytorch-datasets-and-fuse-the-output">
     4. Zip multiple PyTorch datasets and fuse the output
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#patchdataset">
     5. PatchDataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predefined-datasets-for-public-medical-data">
     6. Predefined Datasets for public medical data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partition-dataset-for-cross-validation">
     7. Partition dataset for cross validation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#csv-dataset-and-iterabledataset">
     8. CSV
     <code class="docutils literal notranslate">
      <span class="pre">
       Dataset
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       IterableDataset
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#losses">
   Losses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizers">
   Optimizers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#network-architectures">
   Network architectures
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predefined-layers-and-blocks">
     1. Predefined layers and blocks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation-of-generic-2d-3d-networks">
     2. Implementation of generic 2D/3D networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#network-adapter-to-finetune-final-layers">
     3. Network adapter to finetune final layers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation">
   Evaluation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sliding-window-inference">
     1. Sliding window inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metrics-for-medical-tasks">
     2. Metrics for medical tasks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metrics-report-generation">
     3. Metrics report generation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visualization">
   Visualization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#result-writing">
   Result writing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#workflows">
   Workflows
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-workflows-pipeline">
     1. General workflows pipeline
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ensembleevaluator">
     2. EnsembleEvaluator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transfer-learning-for-different-input-output-classes">
     3. Transfer learning for different input / output classes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transfer-learning-based-on-nvidia-clara-mmar">
     4. Transfer learning based on NVIDIA Clara MMAR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decollate-batch-data-for-flexible-postprocessings">
     5. Decollate batch data for flexible postprocessings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#easy-to-integrate-into-popular-workflows">
     6. Easy to integrate into popular workflows
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#research">
   Research
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cople-net-for-covid-19-pneumonia-lesion-segmentation">
     1. COPLE-Net for COVID-19 Pneumonia Lesion Segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lamp-large-deep-nets-with-automated-model-parallelism-for-image-segmentation">
     2. LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gpu-acceleration">
   GPU acceleration
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#auto-mixed-precision-amp">
     1. Auto mixed precision(AMP)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#distributed-data-parallel">
     2. Distributed data parallel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-cuda-optimized-modules">
     3. C++/CUDA optimized modules
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applications">
   Applications
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deepgrow-modules-for-interactive-segmentation">
     1. DeepGrow modules for interactive segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lesion-detection-in-digital-pathology">
     2. Lesion detection in digital pathology
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-based-image-registration">
     3. Learning-based image registration
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="modules-overview">
<h1>Modules Overview<a class="headerlink" href="#modules-overview" title="Permalink to this headline">¶</a></h1>
<p>MONAI aims at supporting deep learning in medical image analysis at multiple granularities.
This figure shows a typical example of the end-to-end workflow:
<img alt="an end to end workflow" src="_images/end_to_end.png" /></p>
<div class="section" id="monai-architecture">
<h2>MONAI architecture<a class="headerlink" href="#monai-architecture" title="Permalink to this headline">¶</a></h2>
<p>The design principle of MONAI is to provide flexible and light APIs for users with varying expertise.</p>
<ol class="simple">
<li><p>All the core components are independent modules, which can be easily integrated into any existing PyTorch program.</p></li>
<li><p>Users can leverage the workflows in MONAI to quickly set up a robust training or evaluation program for research experiments.</p></li>
<li><p>Rich examples and demos are provided to demonstrate the key features.</p></li>
<li><p>Researchers contribute implementations based on the state-of-the-art for the latest research challenges, including COVID-19 image analysis, Model Parallel, etc.</p></li>
</ol>
<p>The overall architecture and modules are shown in the following figure:
<img alt="architecture overview" src="_images/arch_modules_v0.4.png" />
The rest of this page provides more details for each module.</p>
<ul class="simple">
<li><p><a class="reference external" href="#medical-image-data-i-o-processing-and-augmentation">Data I/O, processing and augmentation</a></p></li>
<li><p><a class="reference external" href="#datasets">Datasets</a></p></li>
<li><p><a class="reference external" href="#losses">Loss functions</a></p></li>
<li><p><a class="reference external" href="#optimizers">Optimizers</a></p></li>
<li><p><a class="reference external" href="#network-architectures">Network architectures</a></p></li>
<li><p><a class="reference external" href="#evaluation">Evaluation</a></p></li>
<li><p><a class="reference external" href="#visualization">Visualization</a></p></li>
<li><p><a class="reference external" href="#result-writing">Result writing</a></p></li>
<li><p><a class="reference external" href="#workflows">Workflows</a></p></li>
<li><p><a class="reference external" href="#research">Research</a></p></li>
<li><p><a class="reference external" href="#gpu-acceleration">GPU acceleration</a></p></li>
<li><p><a class="reference external" href="#applications">Applications</a></p></li>
</ul>
</div>
<div class="section" id="medical-image-data-i-o-processing-and-augmentation">
<h2>Medical image data I/O, processing and augmentation<a class="headerlink" href="#medical-image-data-i-o-processing-and-augmentation" title="Permalink to this headline">¶</a></h2>
<p>Medical images require highly specialized methods for I/O, preprocessing, and augmentation. Medical images are often in specialized formats with rich meta-information, and the data volumes are often high-dimensional. These require carefully designed manipulation procedures. The medical imaging focus of MONAI is enabled by powerful and flexible image transformations that facilitate user-friendly, reproducible, optimized medical data pre-processing pipelines.</p>
<div class="section" id="transforms-support-both-dictionary-and-array-format-data">
<h3>1. Transforms support both Dictionary and Array format data<a class="headerlink" href="#transforms-support-both-dictionary-and-array-format-data" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The widely used computer vision packages (such as <code class="docutils literal notranslate"><span class="pre">torchvision</span></code>) focus on spatially 2D array image processing. MONAI provides more domain-specific transformations for both spatially 2D and 3D and retains the flexible transformation “compose” feature.</p></li>
<li><p>As medical image preprocessing often requires additional fine-grained system parameters, MONAI provides transforms for input data encapsulated in python dictionaries. Users can specify the keys corresponding to the expected data fields and system parameters to compose complex transformations.</p></li>
</ul>
<p>There is a rich set of transforms in six categories: Crop &amp; Pad, Intensity, IO, Post-processing, Spatial, and Utilities. For more details, please visit <a class="reference external" href="https://docs.monai.io/en/latest/transforms.html">all the transforms in MONAI</a>.</p>
<p>Almost all the transforms expect the input data to have a channel-first shape format: <code class="docutils literal notranslate"><span class="pre">[Channel</span> <span class="pre">dim,</span> <span class="pre">spatial</span> <span class="pre">dim</span> <span class="pre">1,</span> <span class="pre">spatial</span> <span class="pre">dim</span> <span class="pre">2,</span> <span class="pre">...]</span></code>.
Flexible <a class="reference external" href="https://github.com/Project-MONAI/MONAI/tree/dev/monai/transforms">base APIs</a> are also provided. The <code class="docutils literal notranslate"><span class="pre">monai.transforms</span></code> module is
easily extensible.</p>
</div>
<div class="section" id="medical-specific-transforms">
<h3>2. Medical specific transforms<a class="headerlink" href="#medical-specific-transforms" title="Permalink to this headline">¶</a></h3>
<p>MONAI aims at providing a comprehensive medical image specific
transformations. These currently include, for example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">LoadImage</span></code>:  Load medical specific formats file from provided path</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Spacing</span></code>:  Resample input image into the specified <code class="docutils literal notranslate"><span class="pre">pixdim</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Orientation</span></code>: Change the image’s orientation into the specified <code class="docutils literal notranslate"><span class="pre">axcodes</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RandGaussianNoise</span></code>: Perturb image intensities by adding statistical noises</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NormalizeIntensity</span></code>: Intensity Normalization based on mean and standard deviation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Affine</span></code>: Transform image based on the affine parameters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Rand2DElastic</span></code>: Random elastic deformation and affine in 2D</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Rand3DElastic</span></code>: Random elastic deformation and affine in 3D</p></li>
</ul>
<p><a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/transforms_demo_2d.ipynb">2D transforms tutorial</a> shows the detailed usage of several MONAI medical image specific transforms.
<img alt="2d transform examples" src="_images/medical_transforms.png" /></p>
</div>
<div class="section" id="fused-spatial-transforms-and-gpu-acceleration">
<h3>3. Fused spatial transforms and GPU acceleration<a class="headerlink" href="#fused-spatial-transforms-and-gpu-acceleration" title="Permalink to this headline">¶</a></h3>
<p>As medical image volumes are usually large (in multi-dimensional arrays), pre-processing performance affects the overall pipeline speed. MONAI provides affine transforms to execute fused spatial operations, supports GPU acceleration via native PyTorch for high performance.</p>
<p>For example:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># create an Affine transform</span>
<span class="n">affine</span> <span class="o">=</span> <span class="n">Affine</span><span class="p">(</span>
    <span class="n">rotate_params</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">scale_params</span><span class="o">=</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">),</span>
    <span class="n">translate_params</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span>
    <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="p">)</span>
<span class="c1"># convert the image using bilinear interpolation</span>
<span class="n">new_img</span> <span class="o">=</span> <span class="n">affine</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">spatial_size</span><span class="o">=</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Experiments and test results are available at <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/acceleration/transform_speed.ipynb">Fused transforms test</a>.</p>
<p>Currently, all the geometric image transforms (Spacing, Zoom, Rotate, Resize, etc.) are designed based on the PyTorch native interfaces. <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/3d_image_transforms.ipynb">Geometric transforms tutorial</a> indicates the usage of affine transforms with 3D medical images.
<img alt="3d transform examples" src="_images/affine.png" /></p>
</div>
<div class="section" id="randomly-crop-out-batch-images-based-on-positive-negative-ratio">
<h3>4. Randomly crop out batch images based on positive/negative ratio<a class="headerlink" href="#randomly-crop-out-batch-images-based-on-positive-negative-ratio" title="Permalink to this headline">¶</a></h3>
<p>Medical image data volume may be too large to fit into GPU memory. A widely-used approach is to randomly draw small size data samples during training and run a “sliding window” routine for inference.  MONAI currently provides general random sampling strategies including class-balanced fixed ratio sampling which may help stabilize the patch-based training process. A typical example is in <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb">Spleen 3D segmentation tutorial</a>, which achieves the class-balanced sampling with <code class="docutils literal notranslate"><span class="pre">RandCropByPosNegLabel</span></code> transform.</p>
</div>
<div class="section" id="deterministic-training-for-reproducibility">
<h3>5. Deterministic training for reproducibility<a class="headerlink" href="#deterministic-training-for-reproducibility" title="Permalink to this headline">¶</a></h3>
<p>Deterministic training support is necessary and important for deep learning research, especially in the medical field. Users can easily set the random seed to all the random transforms in MONAI locally and will not affect other non-deterministic modules in the user’s program.</p>
<p>For example:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># define a transform chain for pre-processing</span>
<span class="n">train_transforms</span> <span class="o">=</span> <span class="n">monai</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">LoadImaged</span><span class="p">(</span><span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">]),</span>
    <span class="n">RandRotate90d</span><span class="p">(</span><span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">],</span> <span class="n">prob</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">spatial_axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
    <span class="o">...</span> <span class="o">...</span>
<span class="p">])</span>
<span class="c1"># set determinism for reproducibility</span>
<span class="n">train_transforms</span><span class="o">.</span><span class="n">set_random_state</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Users can also enable/disable deterministic at the beginning of training program:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">monai</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">set_determinism</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">additional_settings</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="multiple-transform-chains">
<h3>6. Multiple transform chains<a class="headerlink" href="#multiple-transform-chains" title="Permalink to this headline">¶</a></h3>
<p>To apply different transforms on the same data and concatenate the results, MONAI provides <code class="docutils literal notranslate"><span class="pre">CopyItems</span></code> transform to make copies of specified items in the data dictionary and <code class="docutils literal notranslate"><span class="pre">ConcatItems</span></code> transform to combine specified items on the expected dimension, and also provides <code class="docutils literal notranslate"><span class="pre">DeleteItems</span></code> transform to delete unnecessary items to save memory.</p>
<p>Typical usage is to scale the intensity of the same image into different ranges and concatenate the results together.
<img alt="multiple transform chains" src="_images/multi_transform_chains.png" /></p>
</div>
<div class="section" id="debug-transforms-with-datastats">
<h3>7. Debug transforms with DataStats<a class="headerlink" href="#debug-transforms-with-datastats" title="Permalink to this headline">¶</a></h3>
<p>When transforms are combined with the “compose” function, it’s not easy to track the output of a specific transform. To help debug errors in the composed transforms, MONAI provides utility transforms such as <code class="docutils literal notranslate"><span class="pre">DataStats</span></code> to print out intermediate data properties such as <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">shape</span></code>, <code class="docutils literal notranslate"><span class="pre">value</span> <span class="pre">range</span></code>, <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">value</span></code>, <code class="docutils literal notranslate"><span class="pre">Additional</span> <span class="pre">information</span></code>, etc. It’s a self-contained transform and can be integrated into any transform chain.</p>
</div>
<div class="section" id="post-processing-transforms-for-model-output">
<h3>8. Post-processing transforms for model output<a class="headerlink" href="#post-processing-transforms-for-model-output" title="Permalink to this headline">¶</a></h3>
<p>MONAI also provides post-processing transforms for handling the model outputs. Currently, the transforms include:</p>
<ul class="simple">
<li><p>Adding an activation layer (Sigmoid, Softmax, etc.).</p></li>
<li><p>Converting to discrete values (Argmax, One-Hot, Threshold value, etc), as below figure (b).</p></li>
<li><p>Splitting multi-channel data into multiple single channels.</p></li>
<li><p>Removing segmentation noise based on Connected Component Analysis, as below figure (c).</p></li>
<li><p>Extracting contour of segmentation result, which can be used to map to original image and evaluate the model, as below figure (d) and (e).</p></li>
</ul>
<p>After decollating the batch data of model output and applying the post-processing transforms, it’s easier to compute metrics, save model output into files or visualize data in the TensorBoard. <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/postprocessing_transforms.ipynb">Postprocessing transforms tutorial</a> shows an example with several main transforms for post-processing.
<img alt="post-processing transforms" src="_images/postprocessing_transforms.png" /></p>
</div>
<div class="section" id="integrate-third-party-transforms">
<h3>9. Integrate third-party transforms<a class="headerlink" href="#integrate-third-party-transforms" title="Permalink to this headline">¶</a></h3>
<p>The design of MONAI transforms emphasis code readability and usability. It works for array data or dictionary-based data. MONAI also provides <code class="docutils literal notranslate"><span class="pre">Adaptor</span></code> tools to accommodate different data format for 3rd party transforms. To convert the data shapes or types, utility transforms such as <code class="docutils literal notranslate"><span class="pre">ToTensor</span></code>, <code class="docutils literal notranslate"><span class="pre">ToNumpy</span></code>, <code class="docutils literal notranslate"><span class="pre">SqueezeDim</span></code> are also provided. So it’s easy to enhance the transform chain by seamlessly integrating transforms from external packages, including: <code class="docutils literal notranslate"><span class="pre">ITK</span></code>, <code class="docutils literal notranslate"><span class="pre">BatchGenerator</span></code>, <code class="docutils literal notranslate"><span class="pre">TorchIO</span></code> and <code class="docutils literal notranslate"><span class="pre">Rising</span></code>.</p>
<p>For more details, please check out the tutorial: <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/integrate_3rd_party_transforms.ipynb">integrate 3rd party transforms into MONAI program</a>.</p>
</div>
<div class="section" id="io-factory-for-medical-image-formats">
<h3>10. IO factory for medical image formats<a class="headerlink" href="#io-factory-for-medical-image-formats" title="Permalink to this headline">¶</a></h3>
<p>Many popular image formats exist in the medical domain, and they are quite different with rich metadata information. To easily handle different medical image formats in the same pipeline, <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/load_medical_images.ipynb">MONAI provides <code class="docutils literal notranslate"><span class="pre">LoadImage</span></code> transform</a>, which can automatically choose image readers based on the supported suffixes and in the following priority order:</p>
<ul class="simple">
<li><p>User-specified reader at runtime when calling this loader.</p></li>
<li><p>Registered readers from the latest to the first in the list.</p></li>
<li><p>Default readers: (nii, nii.gz -&gt; NibabelReader), (png, jpg, bmp -&gt; PILReader), (npz, npy -&gt; NumpyReader), (others -&gt; ITKReader).</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">ImageReader</span></code> API is quite straightforward, users can easily extend it for their customized image readers.</p>
<p>With these pre-defined image readers, MONAI can load images in formats: <code class="docutils literal notranslate"><span class="pre">NIfTI</span></code>, <code class="docutils literal notranslate"><span class="pre">DICOM</span></code>, <code class="docutils literal notranslate"><span class="pre">PNG</span></code>, <code class="docutils literal notranslate"><span class="pre">JPG</span></code>, <code class="docutils literal notranslate"><span class="pre">BMP</span></code>, <code class="docutils literal notranslate"><span class="pre">NPY/NPZ</span></code>, etc.</p>
</div>
<div class="section" id="save-transform-data-into-nifti-or-png-files">
<h3>11. Save transform data into NIfTI or PNG files<a class="headerlink" href="#save-transform-data-into-nifti-or-png-files" title="Permalink to this headline">¶</a></h3>
<p>To convert images into files or debug the transform chain, MONAI provides <code class="docutils literal notranslate"><span class="pre">SaveImage</span></code> transform. Users can inject this transform into the transform chain to save the results.</p>
</div>
<div class="section" id="automatically-ensure-channel-first-data-shape">
<h3>12. Automatically ensure <code class="docutils literal notranslate"><span class="pre">channel-first</span></code> data shape<a class="headerlink" href="#automatically-ensure-channel-first-data-shape" title="Permalink to this headline">¶</a></h3>
<p>Medical images have different shape formats. They can be <code class="docutils literal notranslate"><span class="pre">channel-last</span></code>, <code class="docutils literal notranslate"><span class="pre">channel-first</span></code> or even <code class="docutils literal notranslate"><span class="pre">no-channel</span></code>. We may, for example, want to load several <code class="docutils literal notranslate"><span class="pre">no-channel</span></code> images and stack them as <code class="docutils literal notranslate"><span class="pre">channel-first</span></code> data. To improve the user experience, MONAI provided an <code class="docutils literal notranslate"><span class="pre">EnsureChannelFirst</span></code> transform to automatically detect data shape according to the meta information and convert it to the <code class="docutils literal notranslate"><span class="pre">channel-first</span></code> format consistently.</p>
</div>
<div class="section" id="invert-spatial-transforms-and-test-time-augmentations">
<h3>13. Invert spatial transforms and test-time augmentations<a class="headerlink" href="#invert-spatial-transforms-and-test-time-augmentations" title="Permalink to this headline">¶</a></h3>
<p>It is often desirable to invert the previously applied spatial transforms (resize, flip, rotate, zoom, crop, pad, etc.) within the deep learning workflows, for example, to resume to the original imaging space after processing the image data in a normalized data space.  Many spatial transforms are enhanced with an <code class="docutils literal notranslate"><span class="pre">inverse</span></code> operation since in v0.5. The <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/torch/unet_inference_dict.py">model inference tutorial</a> shows a basic example.</p>
<p>If the pipeline includes random transformations, users may want to observe the effect that these transformations have on the output. The typical approach is that we pass the same input through the transforms multiple times with different random realizations. Then use the inverse transforms to move all the results to a common space, and calculate the metrics. MONAI provided <code class="docutils literal notranslate"><span class="pre">TestTimeAugmentation</span></code> for this feature, which by default will calculate the <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">standard</span> <span class="pre">deviation</span></code> and <code class="docutils literal notranslate"><span class="pre">volume</span> <span class="pre">variation</span> <span class="pre">coefficient</span></code>.</p>
<p><a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/inverse_transforms_and_test_time_augmentations.ipynb">Invert transforms and TTA tutorials</a> introduce details about the API with usage examples.</p>
<p>(1) The last column is the inverted data of model output:
<img alt="invert transform" src="_images/invert_transforms.png" /></p>
<p>(2) The TTA results of <code class="docutils literal notranslate"><span class="pre">mode</span></code>, <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">standard</span> <span class="pre">deviation</span></code>:
<img alt="test time augmentation" src="_images/tta.png" /></p>
</div>
</div>
<div class="section" id="datasets">
<h2>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h2>
<div class="section" id="cache-io-and-transforms-data-to-accelerate-training">
<h3>1. Cache IO and transforms data to accelerate training<a class="headerlink" href="#cache-io-and-transforms-data-to-accelerate-training" title="Permalink to this headline">¶</a></h3>
<p>Users often need to train the model with many (potentially thousands of) epochs over the data to achieve the desired model quality. A native PyTorch implementation may repeatedly load data and run the same preprocessing steps for every epoch during training, which can be time-consuming and unnecessary, especially when the medical image volumes are large.</p>
<p>MONAI provides a multi-thread <code class="docutils literal notranslate"><span class="pre">CacheDataset</span></code> and <code class="docutils literal notranslate"><span class="pre">LMDBDataset</span></code> to accelerate these transformation steps during training by storing the intermediate outcomes before the first randomized transform in the transform chain. Enabling this feature could potentially give 10x training speedups in the <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/acceleration/dataset_type_performance.ipynb">Datasets experiment</a>.
<img alt="digital pathology" src="_images/cache_dataset.png" /></p>
</div>
<div class="section" id="cache-intermediate-outcomes-into-persistent-storage">
<h3>2. Cache intermediate outcomes into persistent storage<a class="headerlink" href="#cache-intermediate-outcomes-into-persistent-storage" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">PersistentDataset</span></code> is similar to the CacheDataset, where the intermediate cache values are persisted to disk storage or LMDB for rapid retrieval between experimental runs (as is the case when tuning hyperparameters), or when the entire data set size exceeds available memory. The <code class="docutils literal notranslate"><span class="pre">PersistentDataset</span></code> could achieve similar performance when comparing to <code class="docutils literal notranslate"><span class="pre">CacheDataset</span></code> in <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/acceleration/dataset_type_performance.ipynb">Datasets experiment</a>.
<img alt="cachedataset speed" src="_images/datasets_speed.png" /></p>
</div>
<div class="section" id="smartcache-mechanism-for-big-datasets">
<h3>3. SmartCache mechanism for big datasets<a class="headerlink" href="#smartcache-mechanism-for-big-datasets" title="Permalink to this headline">¶</a></h3>
<p>During training with large volume dataset, an efficient approach is to only train with a subset of the dataset in an epoch and dynamically replace part of the subset in every epoch. It’s the <code class="docutils literal notranslate"><span class="pre">SmartCache</span></code> mechanism in <a class="reference external" href="https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/additional_features/smart_cache.html#smart-cache">NVIDIA Clara-train SDK</a>.</p>
<p>MONAI provides a PyTorch version <code class="docutils literal notranslate"><span class="pre">SmartCache</span></code> as <code class="docutils literal notranslate"><span class="pre">SmartCacheDataset</span></code>. In each epoch, only the items in the cache are used for training, at the same time, another thread is preparing replacement items by applying the transform sequence to items not in the cache. Once one epoch is completed, <code class="docutils literal notranslate"><span class="pre">SmartCache</span></code> replaces the same number of items with replacement items.</p>
<p>For example, if we have 5 images: <code class="docutils literal notranslate"><span class="pre">[image1,</span> <span class="pre">image2,</span> <span class="pre">image3,</span> <span class="pre">image4,</span> <span class="pre">image5]</span></code>, and <code class="docutils literal notranslate"><span class="pre">cache_num=4</span></code>, <code class="docutils literal notranslate"><span class="pre">replace_rate=0.25</span></code>. So the actual training images cached and replaced for every epoch are as below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epoch</span> <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="n">image1</span><span class="p">,</span> <span class="n">image2</span><span class="p">,</span> <span class="n">image3</span><span class="p">,</span> <span class="n">image4</span><span class="p">]</span>
<span class="n">epoch</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="n">image2</span><span class="p">,</span> <span class="n">image3</span><span class="p">,</span> <span class="n">image4</span><span class="p">,</span> <span class="n">image5</span><span class="p">]</span>
<span class="n">epoch</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="n">image3</span><span class="p">,</span> <span class="n">image4</span><span class="p">,</span> <span class="n">image5</span><span class="p">,</span> <span class="n">image1</span><span class="p">]</span>
<span class="n">epoch</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="n">image4</span><span class="p">,</span> <span class="n">image5</span><span class="p">,</span> <span class="n">image1</span><span class="p">,</span> <span class="n">image2</span><span class="p">]</span>
<span class="n">epoch</span> <span class="n">N</span><span class="p">:</span> <span class="p">[</span><span class="n">image</span><span class="p">[</span><span class="n">N</span> <span class="o">%</span> <span class="mi">5</span><span class="p">]</span> <span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
<p>Full example of <code class="docutils literal notranslate"><span class="pre">SmartCacheDataset</span></code> is available at <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/acceleration/distributed_training/unet_training_smartcache.py">Distributed training with SmartCache</a>.</p>
</div>
<div class="section" id="zip-multiple-pytorch-datasets-and-fuse-the-output">
<h3>4. Zip multiple PyTorch datasets and fuse the output<a class="headerlink" href="#zip-multiple-pytorch-datasets-and-fuse-the-output" title="Permalink to this headline">¶</a></h3>
<p>MONAI provides <code class="docutils literal notranslate"><span class="pre">ZipDataset</span></code> to associate multiple PyTorch datasets and combine the output data (with the same corresponding batch index) into a tuple, which can be helpful to execute complex training processes based on various data sources.</p>
<p>For example:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DatasetA</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">image_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">DatasetB</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">extra_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">ZipDataset</span><span class="p">([</span><span class="n">DatasetA</span><span class="p">(),</span> <span class="n">DatasetB</span><span class="p">()],</span> <span class="n">transform</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="patchdataset">
<h3>5. PatchDataset<a class="headerlink" href="#patchdataset" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">monai.data.PatchDataset</span></code> provides a flexible API to combine both image- and patch-level preprocessing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">image_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">input_images</span><span class="p">,</span> <span class="n">transforms</span><span class="o">=</span><span class="n">image_transforms</span><span class="p">)</span>
<span class="n">patch_dataset</span> <span class="o">=</span> <span class="n">PatchDataset</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">image_dataset</span><span class="p">,</span> <span class="n">patch_func</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span>
    <span class="n">samples_per_image</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">patch_transforms</span><span class="p">)</span>
</pre></div>
</div>
<p>It supports user-specified <code class="docutils literal notranslate"><span class="pre">image_transforms</span></code> and <code class="docutils literal notranslate"><span class="pre">patch_transforms</span></code> with customisable patch sampling strategies,
which decouples the two-level computations in a multiprocess context.</p>
</div>
<div class="section" id="predefined-datasets-for-public-medical-data">
<h3>6. Predefined Datasets for public medical data<a class="headerlink" href="#predefined-datasets-for-public-medical-data" title="Permalink to this headline">¶</a></h3>
<p>To quickly get started with popular training data in the medical domain, MONAI provides several data-specific Datasets(like: <code class="docutils literal notranslate"><span class="pre">MedNISTDataset</span></code>, <code class="docutils literal notranslate"><span class="pre">DecathlonDataset</span></code>, etc.), which include downloading from our AWS storage, extracting data files and support generation of training/evaluation items with transforms. And they are flexible in that users can easily modify the JSON config file to change the default behaviors.</p>
<p>MONAI always welcome new contributions of public datasets, please refer to existing Datasets and leverage the download and extracting APIs, etc. <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/public_datasets.ipynb">Public datasets tutorial</a> indicates how to quickly set up training workflows with <code class="docutils literal notranslate"><span class="pre">MedNISTDataset</span></code> and <code class="docutils literal notranslate"><span class="pre">DecathlonDataset</span></code> and how to create a new <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> for public data.</p>
<p>The common workflow of predefined datasets:
<img alt="pre-defined dataset" src="_images/dataset_progress.png" /></p>
</div>
<div class="section" id="partition-dataset-for-cross-validation">
<h3>7. Partition dataset for cross validation<a class="headerlink" href="#partition-dataset-for-cross-validation" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">partition_dataset</span></code> utility in MONAI can perform different types of partitioning for training and validation or cross-validation. It supports shuffling based on a specified random seed, and will return a set of datasets, each dataset contains one partition. And it can split the dataset based on specified ratios or evenly split into <code class="docutils literal notranslate"><span class="pre">num_partitions</span></code>. For given class labels, it can also make sure the same ratio of classes in every partition.</p>
</div>
<div class="section" id="csv-dataset-and-iterabledataset">
<h3>8. CSV <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">IterableDataset</span></code><a class="headerlink" href="#csv-dataset-and-iterabledataset" title="Permalink to this headline">¶</a></h3>
<p>CSV tables are often used in additional to image data to incorporate adjunct information, such as patient demographics, lab results, image acquisition parameters and other non-image data, MONAI provides <code class="docutils literal notranslate"><span class="pre">CSVDataset</span></code> to load CSV files and <code class="docutils literal notranslate"><span class="pre">CSVIterableDataset</span></code> to load large CSV files with scalable data access.
In addition to the regular preprocessing transform while loading, it also supports multiple CSV files loading, joining tables, rows and columns selection and grouping. <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/csv_datasets.ipynb">CSVDatasets tutorial</a> shows detailed usage examples.</p>
</div>
</div>
<div class="section" id="losses">
<h2>Losses<a class="headerlink" href="#losses" title="Permalink to this headline">¶</a></h2>
<p>There are domain-specific loss functions in the medical imaging research which are not typically used in generic computer vision tasks. As an important module of MONAI, these loss functions are implemented in PyTorch, such as <code class="docutils literal notranslate"><span class="pre">DiceLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">GeneralizedDiceLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">MaskedDiceLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">TverskyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">FocalLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">DiceCELoss</span></code>, and <code class="docutils literal notranslate"><span class="pre">DiceFocalLoss</span></code>, etc.</p>
</div>
<div class="section" id="optimizers">
<h2>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this headline">¶</a></h2>
<p>MONAI provides several advanced features in optimizers to help accelerate the training or fine-tuning progress. For example, <code class="docutils literal notranslate"><span class="pre">Novograd</span></code> optimizer can be used to converge faster than the traditional optimizers. And users can easily define different learning rates for the model layers based <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/layer_wise_learning_rate.ipynb">on the <code class="docutils literal notranslate"><span class="pre">generate_param_groups</span></code> utility API</a>.</p>
<p>Another important feature is <code class="docutils literal notranslate"><span class="pre">LearningRateFinder</span></code>. The learning rate range test increases the learning rate in a pre-training run between two boundaries in a linear or exponential manner. It provides valuable information on how well the network can be trained over a range of learning rates and what the optimal learning rates are. <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/learning_rate.ipynb">LearningRateFinder tutorial</a> indicates the API usage examples.
<img alt="learning rate finder plot" src="_images/lr_finder.png" /></p>
</div>
<div class="section" id="network-architectures">
<h2>Network architectures<a class="headerlink" href="#network-architectures" title="Permalink to this headline">¶</a></h2>
<p>Some deep neural network architectures have shown to be particularly effective for medical imaging analysis tasks. MONAI implements reference networks with the aims of both flexibility and code readability.</p>
<div class="section" id="predefined-layers-and-blocks">
<h3>1. Predefined layers and blocks<a class="headerlink" href="#predefined-layers-and-blocks" title="Permalink to this headline">¶</a></h3>
<p>To leverage the common network layers and blocks, MONAI provides several predefined layers and blocks which are compatible with 1D, 2D and 3D networks. Users can easily integrate the layer factories in their customised networks.</p>
<p>For example:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># import MONAI’s layer factory</span>
<span class="kn">from</span> <span class="nn">monai.networks.layers</span> <span class="kn">import</span> <span class="n">Conv</span>

<span class="c1"># adds a transposed convolution layer to the network</span>
<span class="c1"># which is compatible with different spatial dimensions.</span>
<span class="n">name</span><span class="p">,</span> <span class="n">dimension</span> <span class="o">=</span> <span class="n">Conv</span><span class="o">.</span><span class="n">CONVTRANS</span><span class="p">,</span> <span class="mi">3</span>
<span class="n">conv_type</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">[</span><span class="n">name</span><span class="p">,</span> <span class="n">dimension</span><span class="p">]</span>
<span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">,</span> <span class="n">conv_type</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="implementation-of-generic-2d-3d-networks">
<h3>2. Implementation of generic 2D/3D networks<a class="headerlink" href="#implementation-of-generic-2d-3d-networks" title="Permalink to this headline">¶</a></h3>
<p>And there are several 1D/2D/3D-compatible implementations of intermediate blocks and generic networks, such as UNet, DynUNet, DenseNet, GAN, AHNet, VNet, SENet(and SEResNet, SEResNeXt), SegResNet, EfficientNet, Attention-based networks. All the networks can support PyTorch serialization pipeline based on <code class="docutils literal notranslate"><span class="pre">torch.jit.script</span></code>.</p>
</div>
<div class="section" id="network-adapter-to-finetune-final-layers">
<h3>3. Network adapter to finetune final layers<a class="headerlink" href="#network-adapter-to-finetune-final-layers" title="Permalink to this headline">¶</a></h3>
<p>Instead of training from scratch, we often leverage the existing models, and finetune the final layers of a network for new learning tasks. MONAI provides a <code class="docutils literal notranslate"><span class="pre">NetAdapter</span></code> to easily replace the last layer of a model by a convolutional layer or a fully-connected layer. A typical usage example is to adapt <a class="reference external" href="https://pytorch.org/vision/stable/models.html">Torchvision models trained with ImageNet</a> for other learning tasks.</p>
</div>
</div>
<div class="section" id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h2>
<p>To run model inferences and evaluate the model quality, MONAI provides reference implementations for the relevant widely-used approaches. Currently, several popular evaluation metrics and inference patterns are included:</p>
<div class="section" id="sliding-window-inference">
<h3>1. Sliding window inference<a class="headerlink" href="#sliding-window-inference" title="Permalink to this headline">¶</a></h3>
<p>For model inferences on large volumes, the sliding window approach is a popular choice to achieve high performance while having flexible memory requirements (<em>alternatively, please check out the latest research on <a class="reference external" href="#lamp-large-deep-nets-with-automated-model-parallelism-for-image-segmentation">model parallel training</a> using MONAI</em>). It also supports <code class="docutils literal notranslate"><span class="pre">overlap</span></code> and <code class="docutils literal notranslate"><span class="pre">blending_mode</span></code> configurations to handle the overlapped windows for better performances.</p>
<p>A typical process is:</p>
<ol class="simple">
<li><p>Select continuous windows on the original image.</p></li>
<li><p>Iteratively run batched window inferences until all windows are analyzed.</p></li>
<li><p>Aggregate the inference outputs to a single segmentation map.</p></li>
<li><p>Save the results to file or compute some evaluation metrics.
<img alt="sliding window scheme" src="_images/sliding_window.png" /></p></li>
</ol>
<p>The <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb">Spleen 3D segmentation tutorial</a> leverages <code class="docutils literal notranslate"><span class="pre">SlidingWindow</span></code> inference for validation.</p>
</div>
<div class="section" id="metrics-for-medical-tasks">
<h3>2. Metrics for medical tasks<a class="headerlink" href="#metrics-for-medical-tasks" title="Permalink to this headline">¶</a></h3>
<p>Various useful evaluation metrics have been used to measure the quality of medical image specific models. MONAI already implemented many medical domain-specific metrics, such as: <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">Dice</span></code>, <code class="docutils literal notranslate"><span class="pre">ROCAUC</span></code>, <code class="docutils literal notranslate"><span class="pre">Confusion</span> <span class="pre">Matrices</span></code>, <code class="docutils literal notranslate"><span class="pre">Hausdorff</span> <span class="pre">Distance</span></code>, <code class="docutils literal notranslate"><span class="pre">Surface</span> <span class="pre">Distance</span></code>, <code class="docutils literal notranslate"><span class="pre">Occlusion</span> <span class="pre">Sensitivity</span></code>.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">Dice</span></code> score can be used for segmentation tasks, and the area under the ROC curve(<code class="docutils literal notranslate"><span class="pre">ROCAUC</span></code>) for classification tasks. We continue to integrate more options.</p>
<ol class="simple">
<li><p>MONAI provides flexible base APIs for metrics
The base classes of MONAI metrics implement the basic computation logic for both iteration and epoch-based metrics. They are a good starting point for customized metrics.</p></li>
<li><p>All the metrics support data parallel computation
With a <code class="docutils literal notranslate"><span class="pre">Cumulative</span></code> base class, intermediate metric outcomes can be automatically buffered, cumulated, synced across distributed processes, and aggregated for the final results. <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/compute_metric.py">Multi-processing computation example</a> shows how to compute metrics based on saved predictions and labels in multi-processing environment.</p></li>
<li><p>All the metrics modules can handle <code class="docutils literal notranslate"><span class="pre">batch-first</span></code> Tensors and list of <code class="docutils literal notranslate"><span class="pre">channel-first</span></code> Tensors</p></li>
</ol>
</div>
<div class="section" id="metrics-report-generation">
<h3>3. Metrics report generation<a class="headerlink" href="#metrics-report-generation" title="Permalink to this headline">¶</a></h3>
<p>During evaluation, users usually save the metrics of every input image, then analyze the bad cases to improve the deep learning pipeline. To save detailed information of metrics, MONAI provided a handler <code class="docutils literal notranslate"><span class="pre">MetricsSaver</span></code>, which can save the final metric values, raw metric of every model output channel of every input image, metrics summary report of operations: <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">median</span></code>, <code class="docutils literal notranslate"><span class="pre">max</span></code>, <code class="docutils literal notranslate"><span class="pre">min</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;int&gt;percentile</span></code>, <code class="docutils literal notranslate"><span class="pre">std</span></code>, etc. The <code class="docutils literal notranslate"><span class="pre">MeanDice</span></code> reports of validation with prostate dataset are as below:
<img alt="metrics report example" src="_images/metrics_report.png" /></p>
</div>
</div>
<div class="section" id="visualization">
<h2>Visualization<a class="headerlink" href="#visualization" title="Permalink to this headline">¶</a></h2>
<p>Beyond the simple point and curve plotting, MONAI provides intuitive interfaces to visualize multidimensional data as GIF animations in TensorBoard. This could provide a quick qualitative assessment of the model by visualizing, for example, the volumetric inputs, segmentation maps, and intermediate feature maps. A runnable example with visualization is available at <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/torch/unet_training_dict.py">UNet training example</a>.</p>
<p>And to visualize the class activation mapping for a trained classification model, MONAI provides CAM, GradCAM, GradCAM++ APIs for both 2D and 3D models:</p>
<p><img alt="CAM visualization example" src="_images/cam.png" /></p>
<p>The above example is generated by computing <a class="reference external" href="https://github.com/Project-MONAI/tutorials/tree/master/modules/interpretability">GradCAM/GradCAM++ from a lung CT lesion classification model</a>.</p>
</div>
<div class="section" id="result-writing">
<h2>Result writing<a class="headerlink" href="#result-writing" title="Permalink to this headline">¶</a></h2>
<p>Currently, MONAI supports writing the model outputs as NIfTI files or PNG files for segmentation tasks, and as CSV files for classification tasks. And the writers can restore the data spacing, orientation or shape according to the <code class="docutils literal notranslate"><span class="pre">original_shape</span></code> or <code class="docutils literal notranslate"><span class="pre">original_affine</span></code> information from the input image.</p>
<p>A rich set of formats will be supported soon, along with relevant statistics and evaluation metrics automatically computed from the outputs.</p>
</div>
<div class="section" id="workflows">
<h2>Workflows<a class="headerlink" href="#workflows" title="Permalink to this headline">¶</a></h2>
<p>To quickly set up training and evaluation experiments, MONAI provides a set of workflows to significantly simplify the modules and allow for fast prototyping.</p>
<p>These features decouple the domain-specific components and the generic machine learning processes. They also provide a set of unify APIs for higher level applications (such as AutoML, Federated Learning).
The trainers and evaluators of the workflows are compatible with pytorch-ignite <code class="docutils literal notranslate"><span class="pre">Engine</span></code> and <code class="docutils literal notranslate"><span class="pre">Event-Handler</span></code> mechanism. There are rich event handlers in MONAI to independently attach to the trainer or evaluator, and users can register additional <code class="docutils literal notranslate"><span class="pre">custom</span> <span class="pre">events</span></code> to workflows.</p>
<div class="section" id="general-workflows-pipeline">
<h3>1. General workflows pipeline<a class="headerlink" href="#general-workflows-pipeline" title="Permalink to this headline">¶</a></h3>
<p>The workflow and some of MONAI event handlers are shown as below:
<img alt="workflow pipeline" src="_images/workflows.png" /></p>
<p>The end-to-end training and evaluation examples are available at <a class="reference external" href="https://github.com/Project-MONAI/tutorials/tree/master/modules/engines">Workflow examples</a>.</p>
</div>
<div class="section" id="ensembleevaluator">
<h3>2. EnsembleEvaluator<a class="headerlink" href="#ensembleevaluator" title="Permalink to this headline">¶</a></h3>
<p>Models ensemble is a popular strategy in machine learning and deep learning areas to achieve more accurate and more stable outputs. A typical practice is:</p>
<ol class="simple">
<li><p>Split all the training dataset into K folds.</p></li>
<li><p>Train K models with every K-1 folds data.</p></li>
<li><p>Execute inference on the test data with all the K models.</p></li>
<li><p>Compute the average values with weights or vote the most common value as the final result.</p></li>
</ol>
<p><img alt="model ensemble" src="_images/models_ensemble.png" />
More details of practice is at <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/models_ensemble.ipynb">Model ensemble tutorial</a>.</p>
</div>
<div class="section" id="transfer-learning-for-different-input-output-classes">
<h3>3. Transfer learning for different input / output classes<a class="headerlink" href="#transfer-learning-for-different-input-output-classes" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Transfer-learning</span></code> is a common and efficient training approach, especially in the medical-specific domain where obtaining large datasets for training can be difficult. So transfer learning from a pre-trained checkpoint can significantly improve the model metrics and shorten training time.</p>
<p>MONAI provided <code class="docutils literal notranslate"><span class="pre">CheckpointLoader</span></code> to load a checkpoint for the workflow before training, and it allows some <code class="docutils literal notranslate"><span class="pre">layer</span> <span class="pre">names</span></code> of the current network don’t match the checkpoint, or some <code class="docutils literal notranslate"><span class="pre">layer</span> <span class="pre">shapes</span></code> don’t match the checkpoint, which can be useful if the current task has different input image classes or output classes.</p>
</div>
<div class="section" id="transfer-learning-based-on-nvidia-clara-mmar">
<h3>4. Transfer learning based on NVIDIA Clara MMAR<a class="headerlink" href="#transfer-learning-based-on-nvidia-clara-mmar" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://docs.nvidia.com/clara/clara-train-sdk/pt/mmar.html">The MMAR (Medical Model ARchive)</a> defines a data structure for organizing all artifacts produced during the model development life cycle. NVIDIA Clara provides rich existing MMARs of medical domain-specific models. And these MMARs include all the information about the model including configurations and scripts to provide a work space to perform all model development tasks. To better leverage the pretrained MMARs released on Nvidia GPU cloud, MONAI provides pythonic APIs to access the MMARs.</p>
<p>The following figure compares the loss curves and validation scores for (1) training from scratch (the green line), (2) applying a pretrained model without training (the magenta line), (3) training from the pretrained model (the blue line), according to the number of training epochs
(the tutorial is available at <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/transfer_mmar.ipynb">transfer_mmar</a>):</p>
<p><img alt="transfer_mmar" src="_images/transfer_mmar.png" /></p>
</div>
<div class="section" id="decollate-batch-data-for-flexible-postprocessings">
<h3>5. Decollate batch data for flexible postprocessings<a class="headerlink" href="#decollate-batch-data-for-flexible-postprocessings" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">decollate</span> <span class="pre">batch</span></code> is introduced in MONAI v0.6, which simplifies the post processing transforms and provides flexible following operations on a batch of data with various data shapes. It can decollate batched data (e.g. model predictions) into a list of tensors, for the benefits such as:</p>
<ol class="simple">
<li><p>enabling postprocessing transforms for each item independently – randomised transforms could be applied differently for each predicted item in a batch.</p></li>
<li><p>simplifying the transform APIs and reducing the input validation burdens because both the preprocessing and postprocessing transforms now only need to support the “channel-first” input format.</p></li>
<li><p>enabling the <code class="docutils literal notranslate"><span class="pre">Invertd</span></code> transform for the predictions and the inverted data with different shapes, as the data items are in a list, not stacked in a single tensor.</p></li>
<li><p>allowing for both batch-first tensor and list of channel-first tensors in a flexible metric computation.</p></li>
</ol>
<p>A typical process of <code class="docutils literal notranslate"><span class="pre">decollate</span> <span class="pre">batch</span></code> is illustrated as follows (with a <code class="docutils literal notranslate"><span class="pre">batch_size=N</span></code> model predictions and labels as an example):
<img alt="decollate_batch" src="_images/decollate_batch.png" /></p>
<p><a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/decollate_batch.ipynb">decollate batch tutorial</a> shows a detailed usage example based on a PyTorch native workflow.</p>
</div>
<div class="section" id="easy-to-integrate-into-popular-workflows">
<h3>6. Easy to integrate into popular workflows<a class="headerlink" href="#easy-to-integrate-into-popular-workflows" title="Permalink to this headline">¶</a></h3>
<p>Except for the pytorch-ignite based <code class="docutils literal notranslate"><span class="pre">monai.engines</span></code>, most of the MONAI modules could be used independently or combined with other software packages. For example, MONAI can be easily integrated into popular frameworks such as PyTorch-Lightning and Catalyst: <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d_lightning.ipynb">Lightning segmentation</a> and <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/modules/TorchIO_MONAI_PyTorch_Lightning.ipynb">Lightning + TorchIO</a> tutorials show the PyTorch Lightning programs with MONAI modules, and <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/unet_segmentation_3d_catalyst.ipynb">Catalyst segmentation</a> shows the Catalyst program with MONAI modules.</p>
</div>
</div>
<div class="section" id="research">
<h2>Research<a class="headerlink" href="#research" title="Permalink to this headline">¶</a></h2>
<p>There are several research prototypes in MONAI corresponding to the recently published papers that address advanced research problems.
We always welcome contributions in forms of comments, suggestions, and code implementations.</p>
<p>The generic patterns/modules identified from the research prototypes will be integrated into MONAI core functionality.</p>
<div class="section" id="cople-net-for-covid-19-pneumonia-lesion-segmentation">
<h3>1. COPLE-Net for COVID-19 Pneumonia Lesion Segmentation<a class="headerlink" href="#cople-net-for-covid-19-pneumonia-lesion-segmentation" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://monai.io/research/coplenet-pneumonia-lesion-segmentation">A reimplementation</a> of the COPLE-Net originally proposed by:</p>
<p>G. Wang, X. Liu, C. Li, Z. Xu, J. Ruan, H. Zhu, T. Meng, K. Li, N. Huang, S. Zhang. (2020) “A Noise-robust Framework for Automatic Segmentation of COVID-19 Pneumonia Lesions from CT Images.” IEEE Transactions on Medical Imaging. 2020. <a class="reference external" href="https://doi.org/10.1109/TMI.2020.3000314">DOI: 10.1109/TMI.2020.3000314</a>
<img alt="coplenet" src="_images/coplenet.png" /></p>
</div>
<div class="section" id="lamp-large-deep-nets-with-automated-model-parallelism-for-image-segmentation">
<h3>2. LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation<a class="headerlink" href="#lamp-large-deep-nets-with-automated-model-parallelism-for-image-segmentation" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://monai.io/research/lamp-automated-model-parallelism">A reimplementation</a> of the LAMP system originally proposed by:</p>
<p>Wentao Zhu, Can Zhao, Wenqi Li, Holger Roth, Ziyue Xu, and Daguang Xu (2020) “LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation.” MICCAI 2020 (Early Accept, paper link: https://arxiv.org/abs/2006.12575)
<img alt="LAMP UNet" src="_images/unet-pipe.png" /></p>
</div>
</div>
<div class="section" id="gpu-acceleration">
<h2>GPU acceleration<a class="headerlink" href="#gpu-acceleration" title="Permalink to this headline">¶</a></h2>
<p>NVIDIA GPUs have been widely applied in many areas of deep learning training and evaluation, and the CUDA parallel computation shows obvious acceleration when comparing to traditional computation methods. To fully leverage GPU features, many popular mechanisms raised, like automatic mixed precision (AMP), distributed data parallel, etc. MONAI can support these features and provides rich examples.</p>
<div class="section" id="auto-mixed-precision-amp">
<h3>1. Auto mixed precision(AMP)<a class="headerlink" href="#auto-mixed-precision-amp" title="Permalink to this headline">¶</a></h3>
<p>In 2017, NVIDIA researchers developed a methodology for mixed-precision training, which combined single-precision (FP32) with half-precision (e.g. FP16) format when training a network, and it achieved the same accuracy as FP32 training using the same hyperparameters.</p>
<p>For the PyTorch 1.6 release, developers at NVIDIA and Facebook moved mixed precision functionality into PyTorch core as the AMP package, <code class="docutils literal notranslate"><span class="pre">torch.cuda.amp</span></code>.</p>
<p>MONAI workflows can easily set <code class="docutils literal notranslate"><span class="pre">amp=True/False</span></code> in <code class="docutils literal notranslate"><span class="pre">SupervisedTrainer</span></code> or <code class="docutils literal notranslate"><span class="pre">SupervisedEvaluator</span></code> during training or evaluation to enable/disable AMP. And we tried to compare the training speed if AMP ON/OFF on NVIDIA V100 GPU with CUDA 11 and PyTorch 1.6, obtained some benchmark results:
<img alt="amp v100 results" src="_images/amp_training_v100.png" />
We also executed the same test program on NVIDIA A100 GPU with the same software environment, obtained faster results:
<img alt="amp a100 results" src="_images/amp_training_a100.png" />
More details is available at <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/acceleration/automatic_mixed_precision.ipynb">AMP training tutorial</a>.
We also tried to combine AMP with <code class="docutils literal notranslate"><span class="pre">CacheDataset</span></code> and <code class="docutils literal notranslate"><span class="pre">Novograd</span></code> optimizer to achieve the fast training in MONAI, able to obtain approximately 12x speedup compared with a Pytorch native implementation when the training converges at a validation mean dice of 0.93. Benchmark for reference:
<img alt="fast training results" src="_images/fast_training.png" />
More details is available at <a class="reference external" href="https://github.com/Project-MONAI/tutorials/blob/master/acceleration/fast_training_tutorial.ipynb">Fast training tutorial</a>.</p>
</div>
<div class="section" id="distributed-data-parallel">
<h3>2. Distributed data parallel<a class="headerlink" href="#distributed-data-parallel" title="Permalink to this headline">¶</a></h3>
<p>Distributed data parallel is an important feature of PyTorch to connect multiple GPU devices on single or multiple nodes to train or evaluate models. The distributed data parallel APIs of MONAI are compatible with native PyTorch distributed module, pytorch-ignite distributed module, Horovod, XLA, and the SLURM platform. MONAI provides demos for reference: train/evaluate with PyTorch DDP, train/evaluate with Horovod, train/evaluate with Ignite DDP, partition dataset and train with SmartCacheDataset, as well as a real world training example based on Decathlon challenge Task01 - Brain Tumor segmentation.  The demo contains distributed caching, training, and validation. We obtained performance benchmarks for reference (based on PyTorch 1.6, CUDA 11, NVIDIA V100 GPUs):</p>
<p><img alt="distributed training results" src="_images/distributed_training.png" /></p>
</div>
<div class="section" id="c-cuda-optimized-modules">
<h3>3. C++/CUDA optimized modules<a class="headerlink" href="#c-cuda-optimized-modules" title="Permalink to this headline">¶</a></h3>
<p>To further accelerate the domain-specific routines in the workflows, MONAI C++/CUDA implementation are introduced as extensions of the PyTorch native implementations.
MONAI provides the modules using <a class="reference external" href="https://pytorch.org/tutorials/advanced/cpp_extension.html#custom-c-and-cuda-extensions">the two ways of building C++ extensions from PyTorch</a>:</p>
<ul class="simple">
<li><p>via <code class="docutils literal notranslate"><span class="pre">setuptools</span></code>, for modules including <code class="docutils literal notranslate"><span class="pre">Resampler</span></code>, <code class="docutils literal notranslate"><span class="pre">Conditional</span> <span class="pre">random</span> <span class="pre">field</span> <span class="pre">(CRF)</span></code>, <code class="docutils literal notranslate"><span class="pre">Fast</span> <span class="pre">bilateral</span> <span class="pre">filtering</span> <span class="pre">using</span> <span class="pre">the</span> <span class="pre">permutohedral</span> <span class="pre">lattice</span></code>.</p></li>
<li><p>via just-in-time (JIT) compilation, for the <code class="docutils literal notranslate"><span class="pre">Gaussian</span> <span class="pre">mixtures</span></code> module. This approach allows for dynamic optimisation according to the user-specified parameters and local system environments.
The following figure shows results of MONAI’s Gaussian mixture models applied to tissue and surgical tools segmentation:
<img alt="Gaussian mixture models as a postprocessing step" src="_images/gmm_feature_set_comparison_s.png" /></p></li>
</ul>
</div>
</div>
<div class="section" id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h2>
<p>The research area of medical image deep learning is expanding fast. To apply the latest achievements into applications, MONAI contains many application components to build end-to-end solutions or prototypes for other similar use cases.</p>
<div class="section" id="deepgrow-modules-for-interactive-segmentation">
<h3>1. DeepGrow modules for interactive segmentation<a class="headerlink" href="#deepgrow-modules-for-interactive-segmentation" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/Project-MONAI/MONAI/tree/master/monai/apps/deepgrow">A reimplementation</a> of the DeepGrow components, which is deep learning based semi-automated segmentation approach that aims to be a “smart” interactive tool for region of interest delineation in medical images, originally proposed by:</p>
<p>Sakinis, Tomas, et al. “Interactive segmentation of medical images through fully convolutional neural networks.” arXiv preprint arXiv:1903.08205 (2019).</p>
<p><img alt="deepgrow scheme" src="_images/deepgrow.png" /></p>
</div>
<div class="section" id="lesion-detection-in-digital-pathology">
<h3>2. Lesion detection in digital pathology<a class="headerlink" href="#lesion-detection-in-digital-pathology" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/Project-MONAI/MONAI/tree/master/monai/apps/pathology">Implementation</a> of the pathology detection components, which includes efficient whole slide imaging IO and sampling with NVIDIA cuCIM library and SmartCache mechanism, FROC measurements for lesion and probabilistic post-processing for lesion detection.</p>
<p><img alt="digital pathology" src="_images/pathology.png" /></p>
</div>
<div class="section" id="learning-based-image-registration">
<h3>3. Learning-based image registration<a class="headerlink" href="#learning-based-image-registration" title="Permalink to this headline">¶</a></h3>
<p>Starting from v0.5.0, MONAI provides experimental features for building learning-based 2D/3D registration workflows.  These include image similarity measures as loss functions, bending energy as model regularization, network architectures, warping modules. The components can be used to build the major unsupervised and weakly-supervised algorithms.</p>
<p>The following figure shows the registration of CT images acquired at different time points for a single patient using MONAI:</p>
<p><img alt="3d registration" src="_images/3d_paired.png" /></p>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="whatsnew_0_5.html" title="previous page">What’s new in 0.5</a>
    <a class='right-next' id="next-link" href="api.html" title="next page">API Reference</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021 MONAI Consortium.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>