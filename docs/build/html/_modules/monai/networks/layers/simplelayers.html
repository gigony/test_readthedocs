
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>monai.networks.layers.simplelayers &#8212; MONAI 0.6.0 Documentation</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../../index.html">
  <img src="../../../../_static/MONAI-logo-color.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../whatsnew.html">
  Whatâ€™s New
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../highlights.html">
  Modules Overview
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../api.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../installation.html">
  Installation Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../contrib.html">
  Development
 </a>
</li>

    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://github.com/Project-MONAI/tutorials">Tutorials<i class="fas fa-external-link-alt"></i></a>
    </li>
    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/project-monai/monai" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/projectmonai" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for monai.networks.layers.simplelayers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020 - 2021 MONAI Consortium</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span>

<span class="kn">from</span> <span class="nn">monai.networks.layers.convutils</span> <span class="kn">import</span> <span class="n">gaussian_1d</span>
<span class="kn">from</span> <span class="nn">monai.networks.layers.factories</span> <span class="kn">import</span> <span class="n">Conv</span>
<span class="kn">from</span> <span class="nn">monai.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">PT_BEFORE_1_7</span><span class="p">,</span>
    <span class="n">ChannelMatching</span><span class="p">,</span>
    <span class="n">InvalidPyTorchVersionError</span><span class="p">,</span>
    <span class="n">SkipMode</span><span class="p">,</span>
    <span class="n">ensure_tuple_rep</span><span class="p">,</span>
    <span class="n">optional_import</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">_C</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">optional_import</span><span class="p">(</span><span class="s2">&quot;monai._C&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">PT_BEFORE_1_7</span><span class="p">:</span>
    <span class="n">fft</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">optional_import</span><span class="p">(</span><span class="s2">&quot;torch.fft&quot;</span><span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;SkipConnection&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Flatten&quot;</span><span class="p">,</span>
    <span class="s2">&quot;GaussianFilter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;LLTM&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Reshape&quot;</span><span class="p">,</span>
    <span class="s2">&quot;separable_filtering&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SavitzkyGolayFilter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;HilbertTransform&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ChannelPad&quot;</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="ChannelPad"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.ChannelPad">[docs]</a><span class="k">class</span> <span class="nc">ChannelPad</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expand the input tensor&#39;s channel dimension from length `in_channels` to `out_channels`,</span>
<span class="sd">    by padding or a projection.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ChannelPad.__init__"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.ChannelPad.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">spatial_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ChannelMatching</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">ChannelMatching</span><span class="o">.</span><span class="n">PAD</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            spatial_dims: number of spatial dimensions of the input image.</span>
<span class="sd">            in_channels: number of input channels.</span>
<span class="sd">            out_channels: number of output channels.</span>
<span class="sd">            mode: {``&quot;pad&quot;``, ``&quot;project&quot;``}</span>
<span class="sd">                Specifies handling residual branch and conv branch channel mismatches. Defaults to ``&quot;pad&quot;``.</span>

<span class="sd">                - ``&quot;pad&quot;``: with zero padding.</span>
<span class="sd">                - ``&quot;project&quot;``: with a trainable conv with kernel size one.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">project</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">==</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="n">ChannelMatching</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ChannelMatching</span><span class="o">.</span><span class="n">PROJECT</span><span class="p">:</span>
            <span class="n">conv_type</span> <span class="o">=</span> <span class="n">Conv</span><span class="p">[</span><span class="n">Conv</span><span class="o">.</span><span class="n">CONV</span><span class="p">,</span> <span class="n">spatial_dims</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">project</span> <span class="o">=</span> <span class="n">conv_type</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">ChannelMatching</span><span class="o">.</span><span class="n">PAD</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">in_channels</span> <span class="o">&gt;</span> <span class="n">out_channels</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Incompatible values: channel_matching=&quot;pad&quot; and in_channels &gt; out_channels.&#39;</span><span class="p">)</span>
            <span class="n">pad_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_channels</span> <span class="o">-</span> <span class="n">in_channels</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">pad_2</span> <span class="o">=</span> <span class="n">out_channels</span> <span class="o">-</span> <span class="n">in_channels</span> <span class="o">-</span> <span class="n">pad_1</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">spatial_dims</span> <span class="o">+</span> <span class="p">[</span><span class="n">pad_1</span><span class="p">,</span> <span class="n">pad_2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span>
            <span class="k">return</span></div>

<div class="viewcode-block" id="ChannelPad.forward"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.ChannelPad.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">project</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">project</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># as_tensor used to get around mypy typing bug</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div></div>


<div class="viewcode-block" id="SkipConnection"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.SkipConnection">[docs]</a><span class="k">class</span> <span class="nc">SkipConnection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Combine the forward pass input with the result from the given submodule::</span>

<span class="sd">        --+--submodule--o--</span>
<span class="sd">          |_____________|</span>

<span class="sd">    The available modes are ``&quot;cat&quot;``, ``&quot;add&quot;``, ``&quot;mul&quot;``.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="SkipConnection.__init__"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.SkipConnection.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">submodule</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">SkipMode</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            submodule: the module defines the trainable branch.</span>
<span class="sd">            dim: the dimension over which the tensors are concatenated.</span>
<span class="sd">                Used when mode is ``&quot;cat&quot;``.</span>
<span class="sd">            mode: ``&quot;cat&quot;``, ``&quot;add&quot;``, ``&quot;mul&quot;``. defaults to ``&quot;cat&quot;``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">submodule</span> <span class="o">=</span> <span class="n">submodule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">SkipMode</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span><span class="o">.</span><span class="n">value</span></div>

<div class="viewcode-block" id="SkipConnection.forward"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.SkipConnection.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">submodule</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;cat&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;add&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;mul&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported mode </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Flatten"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.Flatten">[docs]</a><span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flattens the given input in the forward pass to be [B,-1] in shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="Flatten.forward"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.Flatten.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span></div></div>


<span class="k">class</span> <span class="nc">Reshape</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes input tensors to the given shape (minus batch dimension), retaining original batch size.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a shape list/tuple `shape` of integers (s0, s1, ... , sn), this layer will reshape input tensors of</span>
<span class="sd">        shape (batch, s0 * s1 * ... * sn) to shape (batch, s0, s1, ... , sn).</span>

<span class="sd">        Args:</span>
<span class="sd">            shape: list/tuple of integer shape dimensions</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># done this way for Torchscript</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_separable_filtering_conv</span><span class="p">(</span>
    <span class="n">input_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">kernels</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">pad_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">d</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">spatial_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">paddings</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">num_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

    <span class="k">if</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">input_</span>

    <span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">s</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">_kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="p">[</span><span class="n">d</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># if filter kernel is unity, don&#39;t convolve</span>
    <span class="k">if</span> <span class="n">_kernel</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">_kernel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_separable_filtering_conv</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">kernels</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">spatial_dims</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">)</span>

    <span class="n">_kernel</span> <span class="o">=</span> <span class="n">_kernel</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="n">num_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">spatial_dims</span><span class="p">)</span>
    <span class="n">_padding</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">spatial_dims</span>
    <span class="n">_padding</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">paddings</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
    <span class="n">conv_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">][</span><span class="n">spatial_dims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

    <span class="c1"># translate padding for input to torch.nn.functional.pad</span>
    <span class="n">_reversed_padding_repeated_twice</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[[</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">_padding</span><span class="p">)]</span>
    <span class="n">_sum_reversed_padding_repeated_twice</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="p">[])</span>
    <span class="n">padded_input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">_sum_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">pad_mode</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">conv_type</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">_separable_filtering_conv</span><span class="p">(</span><span class="n">padded_input</span><span class="p">,</span> <span class="n">kernels</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">spatial_dims</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">),</span>
        <span class="n">weight</span><span class="o">=</span><span class="n">_kernel</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">num_channels</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">separable_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">kernels</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply 1-D convolutions along each spatial dimension of `x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: the input image. must have shape (batch, channels, H[, W, ...]).</span>
<span class="sd">        kernels: kernel along each spatial dimension.</span>
<span class="sd">            could be a single kernel (duplicated for all dimension), or `spatial_dims` number of kernels.</span>
<span class="sd">        mode (string, optional): padding mode passed to convolution class. ``&#39;zeros&#39;``, ``&#39;reflect&#39;``, ``&#39;replicate&#39;``</span>
<span class="sd">            or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``. Modes other than ``&#39;zeros&#39;`` require PyTorch version &gt;= 1.5.1. See</span>
<span class="sd">            torch.nn.Conv1d() for more information.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: When ``x`` is not a ``torch.Tensor``.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x must be a torch.Tensor but is </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">spatial_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span>
    <span class="n">_kernels</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">kernels</span><span class="p">]</span>
    <span class="n">_paddings</span> <span class="o">=</span> <span class="p">[(</span><span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">_kernels</span><span class="p">]</span>
    <span class="n">n_chs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">pad_mode</span> <span class="o">=</span> <span class="s2">&quot;constant&quot;</span> <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;zeros&quot;</span> <span class="k">else</span> <span class="n">mode</span>

    <span class="k">return</span> <span class="n">_separable_filtering_conv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernels</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">spatial_dims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">spatial_dims</span><span class="p">,</span> <span class="n">_paddings</span><span class="p">,</span> <span class="n">n_chs</span><span class="p">)</span>


<div class="viewcode-block" id="SavitzkyGolayFilter"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.SavitzkyGolayFilter">[docs]</a><span class="k">class</span> <span class="nc">SavitzkyGolayFilter</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convolve a Tensor along a particular axis with a Savitzky-Golay kernel.</span>

<span class="sd">    Args:</span>
<span class="sd">        window_length: Length of the filter window, must be a positive odd integer.</span>
<span class="sd">        order: Order of the polynomial to fit to each window, must be less than ``window_length``.</span>
<span class="sd">        axis (optional): Axis along which to apply the filter kernel. Default 2 (first spatial dimension).</span>
<span class="sd">        mode (string, optional): padding mode passed to convolution class. ``&#39;zeros&#39;``, ``&#39;reflect&#39;``, ``&#39;replicate&#39;`` or</span>
<span class="sd">        ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``. See torch.nn.Conv1d() for more information.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">window_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">order</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">order</span> <span class="o">&gt;=</span> <span class="n">window_length</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;order must be less than window_length.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coeffs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_coeffs</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>

<div class="viewcode-block" id="SavitzkyGolayFilter.forward"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.SavitzkyGolayFilter.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            x: Tensor or array-like to filter. Must be real, in shape ``[Batch, chns, spatial1, spatial2, ...]`` and</span>
<span class="sd">                have a device type of ``&#39;cpu&#39;``.</span>
<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: ``x`` filtered by Savitzky-Golay kernel with window length ``self.window_length`` using</span>
<span class="sd">            polynomials of order ``self.order``, along axis specified in ``self.axis``.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Make input a real tensor on the CPU</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;x must be real.&quot;</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid axis for shape of x.&quot;</span><span class="p">)</span>

        <span class="c1"># Create list of filter kernels (1 per spatial dimension). The kernel for self.axis will be the savgol coeffs,</span>
        <span class="c1"># while the other kernels will be set to [1].</span>
        <span class="n">n_spatial_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span>
        <span class="n">spatial_processing_axis</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">-</span> <span class="mi">2</span>
        <span class="n">new_dims_before</span> <span class="o">=</span> <span class="n">spatial_processing_axis</span>
        <span class="n">new_dims_after</span> <span class="o">=</span> <span class="n">n_spatial_dims</span> <span class="o">-</span> <span class="n">spatial_processing_axis</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">kernel_list</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">coeffs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">new_dims_before</span><span class="p">):</span>
            <span class="n">kernel_list</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">new_dims_after</span><span class="p">):</span>
            <span class="n">kernel_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">separable_filtering</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_list</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_make_coeffs</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">order</span><span class="p">):</span>

        <span class="n">half_length</span><span class="p">,</span> <span class="n">rem</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rem</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;window_length must be odd.&quot;</span><span class="p">)</span>

        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">window_length</span> <span class="o">-</span> <span class="n">half_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="n">half_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">**</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">solution</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span></div>


<div class="viewcode-block" id="HilbertTransform"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.HilbertTransform">[docs]</a><span class="k">class</span> <span class="nc">HilbertTransform</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine the analytical signal of a Tensor along a particular axis.</span>
<span class="sd">    Requires PyTorch 1.7.0+ and the PyTorch FFT module (which is not included in NVIDIA PyTorch Release 20.10).</span>

<span class="sd">    Args:</span>
<span class="sd">        axis: Axis along which to apply Hilbert transform. Default 2 (first spatial dimension).</span>
<span class="sd">        N: Number of Fourier components (i.e. FFT size). Default: ``x.shape[axis]``.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="k">if</span> <span class="n">PT_BEFORE_1_7</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">InvalidPyTorchVersionError</span><span class="p">(</span><span class="s2">&quot;1.7.0&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span>

<div class="viewcode-block" id="HilbertTransform.forward"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.HilbertTransform.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            x: Tensor or array-like to transform. Must be real and in shape ``[Batch, chns, spatial1, spatial2, ...]``.</span>
<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: Analytical signal of ``x``, transformed along axis specified in ``self.axis`` using</span>
<span class="sd">            FFT of size ``self.N``. The absolute value of ``x_ht`` relates to the envelope of ``x`` along axis ``self.axis``.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Make input a real tensor</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;x must be real.&quot;</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid axis for shape of x.&quot;</span><span class="p">)</span>

        <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
        <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;N must be positive.&quot;</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">complex64</span><span class="p">)</span>
        <span class="c1"># Create frequency axis</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">true_divide</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">)),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">true_divide</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">n</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">)),</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">xf</span> <span class="o">=</span> <span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="c1"># Create step function</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">u</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">new_dims_before</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span>
        <span class="n">new_dims_after</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">xf</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">new_dims_before</span><span class="p">):</span>
            <span class="n">u</span><span class="o">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">new_dims_after</span><span class="p">):</span>
            <span class="n">u</span><span class="o">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">ht</span> <span class="o">=</span> <span class="n">fft</span><span class="o">.</span><span class="n">ifft</span><span class="p">(</span><span class="n">xf</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">u</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>

        <span class="c1"># Apply transform</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">ht</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">ht</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ht</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="GaussianFilter"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.GaussianFilter">[docs]</a><span class="k">class</span> <span class="nc">GaussianFilter</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<div class="viewcode-block" id="GaussianFilter.__init__"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.GaussianFilter.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">spatial_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">sigma</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">truncated</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
        <span class="n">approx</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;erf&quot;</span><span class="p">,</span>
        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            spatial_dims: number of spatial dimensions of the input image.</span>
<span class="sd">                must have shape (Batch, channels, H[, W, ...]).</span>
<span class="sd">            sigma: std. could be a single value, or `spatial_dims` number of values.</span>
<span class="sd">            truncated: spreads how many stds.</span>
<span class="sd">            approx: discrete Gaussian kernel type, available options are &quot;erf&quot;, &quot;sampled&quot;, and &quot;scalespace&quot;.</span>

<span class="sd">                - ``erf`` approximation interpolates the error function;</span>
<span class="sd">                - ``sampled`` uses a sampled Gaussian kernel;</span>
<span class="sd">                - ``scalespace`` corresponds to</span>
<span class="sd">                  https://en.wikipedia.org/wiki/Scale_space_implementation#The_discrete_Gaussian_kernel</span>
<span class="sd">                  based on the modified Bessel functions.</span>

<span class="sd">            requires_grad: whether to store the gradients for sigma.</span>
<span class="sd">                if True, `sigma` will be the initial value of the parameters of this module</span>
<span class="sd">                (for example `parameters()` iterator could be used to get the parameters);</span>
<span class="sd">                otherwise this module will fix the kernels using `sigma` as the std.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">s</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">ensure_tuple_rep</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">spatial_dims</span><span class="p">))</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">truncated</span> <span class="o">=</span> <span class="n">truncated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">approx</span> <span class="o">=</span> <span class="n">approx</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;kernel_sigma_</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span></div>

<div class="viewcode-block" id="GaussianFilter.forward"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.GaussianFilter.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            x: in shape [Batch, chns, H, W, D].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_kernel</span> <span class="o">=</span> <span class="p">[</span><span class="n">gaussian_1d</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">truncated</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">truncated</span><span class="p">,</span> <span class="n">approx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">approx</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">separable_filtering</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">kernels</span><span class="o">=</span><span class="n">_kernel</span><span class="p">)</span></div></div>


<span class="k">class</span> <span class="nc">LLTMFunction</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">old_h</span><span class="p">,</span> <span class="n">old_cell</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">lltm_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">old_h</span><span class="p">,</span> <span class="n">old_cell</span><span class="p">)</span>
        <span class="n">new_h</span><span class="p">,</span> <span class="n">new_cell</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">variables</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">weights</span><span class="p">]</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="n">variables</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_cell</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_h</span><span class="p">,</span> <span class="n">grad_cell</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">lltm_backward</span><span class="p">(</span><span class="n">grad_h</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">grad_cell</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="o">*</span><span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">)</span>
        <span class="n">d_old_h</span><span class="p">,</span> <span class="n">d_input</span><span class="p">,</span> <span class="n">d_weights</span><span class="p">,</span> <span class="n">d_bias</span><span class="p">,</span> <span class="n">d_old_cell</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">d_input</span><span class="p">,</span> <span class="n">d_weights</span><span class="p">,</span> <span class="n">d_bias</span><span class="p">,</span> <span class="n">d_old_h</span><span class="p">,</span> <span class="n">d_old_cell</span>


<div class="viewcode-block" id="LLTM"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.LLTM">[docs]</a><span class="k">class</span> <span class="nc">LLTM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This recurrent unit is similar to an LSTM, but differs in that it lacks a forget</span>
<span class="sd">    gate and uses an Exponential Linear Unit (ELU) as its internal activation function.</span>
<span class="sd">    Because this unit never forgets, call it LLTM, or Long-Long-Term-Memory unit.</span>
<span class="sd">    It has both C++ and CUDA implementation, automatically switch according to the</span>
<span class="sd">    target device where put this module to.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_features: size of input feature data</span>
<span class="sd">        state_size: size of the state of recurrent unit</span>

<span class="sd">    Referring to: https://pytorch.org/tutorials/advanced/cpp_extension.html</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">state_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LLTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">input_features</span> <span class="o">+</span> <span class="n">state_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">state_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">stdv</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">stdv</span><span class="p">,</span> <span class="o">+</span><span class="n">stdv</span><span class="p">)</span>

<div class="viewcode-block" id="LLTM.forward"><a class="viewcode-back" href="../../../../networks.html#monai.networks.layers.LLTM.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">LLTMFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">*</span><span class="n">state</span><span class="p">)</span></div></div>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020 - 2021 MONAI Consortium.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>